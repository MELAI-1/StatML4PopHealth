{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQzwMCE3hyta"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/imperial.png\" width=\"250\" vspace=\"8\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/mlgh.png\" width=\"220\" hspace=\"50\" vspace=\"5\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/ammi.png\" width=\"190\"/>\n",
    "\n",
    "<font size=\"6\">Modern Statistics and Machine Learning<br> for Population Health in Africa </font>\n",
    "\n",
    "<font size=\"4\">24th - 28th March 2025</font>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmnPYY2MTOhq"
   },
   "source": [
    "# Computing lab: Bayesian normal fixed effects regression models\n",
    "## Tristan Naidoo and Sahoko Ishida\n",
    "\n",
    "In this notebook, we will fit a Bayesian normal linear regression model to the windspeed data processed in the notebook `wind_data_cleaning.ipynb`, using the probabilistic computing language Stan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-C6TIbeQil70"
   },
   "source": [
    "### 1. Installing cmdstanpy and loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMyMxvo6niG6"
   },
   "outputs": [],
   "source": [
    "!curl -O \"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/scripts/utilities.py\"\n",
    "\n",
    "from utilities import custom_install_cmdstan, test_cmdstan_installation\n",
    "\n",
    "custom_install_cmdstan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow294tmWY8Aa"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"cmdstanpy\").setLevel(logging.INFO)\n",
    "\n",
    "# Test the cmdstan installation\n",
    "test_cmdstan_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3VMKmc8I-VS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "from cmdstanpy import CmdStanModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "font = {\"family\": \"sans-serif\",\n",
    "        \"weight\": \"normal\",\n",
    "\t\t\"size\": 10.5}\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fAZoePRVgbG"
   },
   "outputs": [],
   "source": [
    "# get the input data\n",
    "!curl -O \"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/day1/practical1/data/cleaned_wind_data_date_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1j3xEnuKx6s"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Adjust this as required - this is where your output will be stored.\n",
    "output_dir = Path(*[\"drive\", \"MyDrive\", \"StatML4PopHealth\", \"practical1\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTmTF2Fhijfd"
   },
   "source": [
    "### 2. Load and visualise data, study objectives\n",
    "\n",
    "Wind speed is influenced by factors like temperature and location. As temperatures rise during the day, wind speeds tend to increase, creating a typical diurnal pattern with higher winds in the afternoon and lower winds at night. Seasonal patterns also emerge as changes in weather and atmospheric conditions vary throughout the year.\n",
    "\n",
    "From the pre-processed dataset, we will take a look at hourly windspeed recordings at a selected site (`'Bothasig AQM Site'`), between 1st January, 2020 and 21st January 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5TneNSrtgSu"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_wind_data_date_features.csv\")\n",
    "df.head()\n",
    "df.timestamp =  pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df.date =  pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrZpECR3u7rr"
   },
   "outputs": [],
   "source": [
    "df_diurnal = df[(df[\"date\"] >= \"2020-01-01\") & (df[\"date\"] <=\"2020-01-21\") &\n",
    " (df[\"location\"]==\"Bothasig AQM Site\")]\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(data=df_diurnal, x=\"timestamp\", y=\"wind_speed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rgGgmaR8tna"
   },
   "source": [
    "### Research questions\n",
    "\n",
    "From the plot above, we can observe some discernible patterns, including a diurnal cycle and an overall trend over the study period. Our objectives are as follows:\n",
    "\n",
    "1. Diurnal Cycle Analysis\n",
    "  - What does the average diurnal cycle of wind speed look like?  \n",
    "  - At what time does wind speed peak, and when does it reach its lowest point?  \n",
    "  - Can we quantify the difference between the maximum and minimum wind speed within a day and estimate its credible interval?  \n",
    "\n",
    "2. Overall Trend Analysis\n",
    "  - The overall trend appears to be non-linear.  \n",
    "  - Can we model this trend by incorporating polynomial terms?  \n",
    "  - What is the most appropriate polynomial order for capturing this trend?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbS-FeRY3RMJ"
   },
   "source": [
    "## 3.Building a statistical model\n",
    "To investigate these questions, we will stick with a Bayesian normal fixed effects regression model.\n",
    "\n",
    "Let us denote by $y_i$ the log windspeed of the $i$th observation; by $X_{ij}$, $j=1,\\dotsc,23$ binary indicators that evaluate to $1$ if the $i$th observation corresponds to the $j$th hour of the day; and by $X_{i25}$ the standardised `day` variable.  We will also include polynomial terms of this variable. We define our regression model as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "& y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n",
    "& \\mu_i = \\beta_0 + \\sum_{j=1}^{24}\\beta_{j} X_{ij} + \\sum_{p=1}^{P} \\beta_{24+p} X_{i25}^{p} \\\\\n",
    "& \\beta_0 \\sim \\text{Normal}(0, 100) \\\\\n",
    "& \\beta_j \\sim \\text{Normal}(0, 1) \\\\\n",
    "& \\sigma \\sim \\text{Half-Cauchy}(0,1)\n",
    "\\end{align*}\n",
    "\n",
    "Here, the $\\beta$ terms are the unknown fixed effect regression coefficients, and $\\sigma^2$ is the measurement noise.\n",
    "\n",
    "We want to estimate the joint posterior density\n",
    "\\begin{align*}\n",
    "p(\\beta_0,\\dotsc,\\beta_{24+P},\\sigma | y, X).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peIgDFZK77Zs"
   },
   "source": [
    "#### 3.1 Consideration\n",
    "* Covariates $X_1, \\ldots, X_{24}$ (hour):\n",
    "The original variable hour is categorical with 24 categories. A common practice is to select one category (e.g., hour 0) as the base and include the remaining 23 dummy variables in the model. However, in this analysis, we choose to include all 24 categories. This approach is motivated by our interest in capturing the average diurnal pattern, which can be visualized by plotting the coefficient estimates $\\hat{\\beta}{1}, \\ldots, \\hat{\\beta}{24}$ along with their credible intervals. Omitting the base category would be inconvenient for this purpose. To ensure that these 24 parameters are identifiable, we impose the constraint that they sum to zero: $\\sum_{j=1}^{24}\\beta_j = 0$. This constraint guarantees identifiability while allowing us to interpret the average diurnal pattern directly from the coefficient estimates.\n",
    "* Covariates $X_{25},\\dots,X_{24+P}$ (day):\n",
    "Let $D_i$ be the variable indicating the day on which the $i$-th observation is recorded, such that $D_i = 1$ if the observation was recorded on the first day of the study period, $D_i = 2$ if recorded on the second day, and so on. We standardise this variable by $X_{i25} = (D_i - m_i)/s_i$, where $m_i$ and $s_i$ are mean and the standard deviation of $D_i$.\n",
    "* The target variable wind_speed: This variable takes positive values. In this analysis, we apply a simple log transformation. Alternatively, we could deviate from the normal model and use a probability distribution with support $(0, \\infty)$, such as the Weibull distribution, which is a common choice for modeling wind speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9QZnk-470uM"
   },
   "source": [
    "#### 3.2 Prior specification\n",
    "\n",
    "To specify the prior distributions, I worked out that the mean log windspeed is around $1.5$. So we expect the posterior $\\beta_0$ to be around that value in the above model. For this reason, we chose a $\\text{Normal}(0, 5^2)$ prior, which has 95% mass approximately in $[-10,10]$. With this choice, we expect that the marginal posterior density of $\\beta_0$ will lie nicely within the prior density of $\\beta_0$. Note that it would be poor practice to specify a prior that is much wider (such as $\\text{Normal}(0, 100^2)$) as this may lead to poor sampling. It would also be poor practice to center the prior at $1.5$ (such as $\\text{Normal}(1.5, 1^2)$) as this could make it difficult to apply the model to a slightly different data set.\n",
    "\n",
    "For the binary variables and the standardised $X_{i25}$ and its polynomials, we can chose default $\\text{Normal}(0, 1)$ priors. If $X_{i25}$ had not been standardised and be in nominal log year or year values, we would expect very small values of the posterior $\\beta_{25}$, and a $\\text{Normal}(0, 1)$ might be unsuitably vague. This should make clear to you that, typically, the default prior choices that are listed and investigated in textbooks or the `Stan` manual are suitable to data and covariates on standardised scales. Once the data are standardised, then there are standard Bayesian recipes that can be applied. Without standardisation, off-the-shelf recommendations are hard to make or may be poorly justified.\n",
    "\n",
    "The measurement noise $\\sigma$ is given a fairly heavy tailed $\\text{Half-Cauchy}(0,1)$ prior density. Other good options might be an Exponential prior density, Half-normal prior, or Inverse-Gamma prior on $\\sigma^2$; please see the `Stan` manual for further discussion.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVgjY38k8hRi"
   },
   "source": [
    "## 4 Numerical inference with Stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAEmO5QO_UNe"
   },
   "source": [
    "### 4.1 Further data processing\n",
    "* Filter the data based on study period and site(s) chosen. You can for example choose `\"Bothasig AQM Site\"`, `start_date = \"2020-01-01\"`, `end_date = \"2020-01-22\"`\n",
    "* Log transform the windspeed. Make sure to remove the entries with NA values. Use `DataFrame.dropna()`\n",
    "* Create dummy variables. You can use `pd.get_dummies()` with an option `drop_first = False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffnvN1Az_WCC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2dZ8MF2_WyY"
   },
   "source": [
    "### 4.2 Stan code\n",
    "Using the exmple code we looked at during the lecture, let's write stan code for the model in Section 3. Make sure to\n",
    "- Standardise the day variable, $X_{25}$\n",
    "- Incorporate a sum-to-zero constraint on $\\beta_{1},\\ldots,\\beta_{24}$. There are multiple ways to do this, and one is to add `soft-sum-to-zero` constraint, as part of prior in the model block. Consider adding `sum(beta_hour) ~ normal( 0 , 0.001*24 )`, where `beta_hour` is a vector of length $24$. What does this mean? It tries to keep the sum as close to zero as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a stan code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHYsbJxJ7zeD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYzf0TfFbTyp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data dicitionary for Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUfgGA-YpTNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the compiled model, with the following setting\n",
    "* 4 chains\n",
    "* warm-up 500\n",
    "* sampling 2000 \n",
    "\n",
    "Make sure to set a seed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLQVBFy0LceU"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHMfNwlWHutO"
   },
   "source": [
    "### 4.3 Convergence and mixing analysis of Stan output\n",
    "Before we analyse the estimated model parameters in detail, we need to check that the algorithm converged and mixed suitably. A quick way to check if there have been any problems is to run `print(model1_fit.diagnose())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2eyWsxKjMbs"
   },
   "outputs": [],
   "source": [
    "print(model1_fit.diagnose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FU6MYe5OWjb"
   },
   "source": [
    "We can also get summary statistics. \n",
    "\n",
    "* What does the column n_eff mean, and are the values reasonably large?\n",
    "* What does the column Rhat mean, and are the values reasonably small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPWeMSsq3Qlf"
   },
   "outputs": [],
   "source": [
    "# We can also use a library to get this summary\n",
    "# Convert the CmdStanPy model fit to ArviZ InferenceData\n",
    "model1_fit_az_idata = az.from_cmdstanpy(model1_fit, save_warmup=True)\n",
    "\n",
    "# Get summary statistics, including 2.75% and 97.5% quantiles\n",
    "summary_stats = az.summary(model1_fit_az_idata,\n",
    "                           var_names=[\"beta0\",\"beta_hour\",\"beta_day\",\"sigma\"],\n",
    "                           group=\"posterior\",\n",
    "                           hdi_prob=0.95,\n",
    "                           kind=\"all\"# 95% highest density interval\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byXF1y1-bAt2"
   },
   "outputs": [],
   "source": [
    "summary_stats.index = ['beta0'] + [f\"hour_{i}\" for i in range(1, 25)] + ['day','squared_day','cubic_day'] + ['sigma']\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQW6Ar5KZeln"
   },
   "source": [
    "The $\\hat{R}$ and $n_{eff}$ statistics are just scalar indicators of convergence and mixing. It is much clearer to diagnose potential numerical issues with the trace plots and pair plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eU0G2PWOVIA"
   },
   "outputs": [],
   "source": [
    "# Trace plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJuHP-n-enFa"
   },
   "outputs": [],
   "source": [
    "# Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1koXkX4e8zl"
   },
   "source": [
    "Take a minute to check the output:\n",
    "\n",
    "* What do the trace plots show? Are the trace plots reasonable?\n",
    "* What do the pair plots show? Are the pair plots reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgQNo7DtLHQA"
   },
   "source": [
    "We can now summarise the posterior distribution of the expected windspeed given the day and hour of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEJTIC-ALKy1"
   },
   "outputs": [],
   "source": [
    "custom_stats = {\n",
    "    \"median\": lambda x: np.percentile(x, 50),\n",
    "\t\"ci_2.5%\": lambda x: np.percentile(x, 2.5),\n",
    "\t\"ci_97.5%\": lambda x: np.percentile(x, 97.5)\n",
    "}\n",
    "# Summarise the posterior of the mu parameter\n",
    "mu_sum = az.summary(model1_fit_az_idata, var_names=['mu'], stat_funcs=custom_stats, extend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are modelling the log of wind speed. Let's modify the code above, to put it back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNXlA5rjgmut"
   },
   "outputs": [],
   "source": [
    "custom_stats_original_sclae = {\n",
    "    \"median_original\": lambda x: np.percentile(np.exp(x), 50),\n",
    "\t\"ci_2.5%_original\": lambda x: np.percentile(np.exp(x), 2.5),\n",
    "\t\"ci_97.5%_original\": lambda x: np.percentile(np.exp(x), 97.5)\n",
    "}\n",
    "# Summarise the posterior of the mu parameter\n",
    "exp_mu_sum = az.summary(model1_fit_az_idata, var_names=['mu'], stat_funcs=custom_stats_original_sclae, extend=False)\n",
    "mu_sum = pd.concat([mu_sum, exp_mu_sum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx_kaxetLc7P"
   },
   "outputs": [],
   "source": [
    "# Append the data to the summary\n",
    "mu_sum['wind_speed'] = df_diurnal['wind_speed'].values\n",
    "mu_sum['log_wind_speed'] = df_diurnal['log_wind_speed'].values\n",
    "mu_sum['timestamp'] = df_diurnal['timestamp'].values\n",
    "\n",
    "# Sort the data by timestamp\n",
    "mu_sum = mu_sum.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqlMmxvvL1MY"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.plot(mu_sum['timestamp'], mu_sum['log_wind_speed'],color='black', alpha=0.5)\n",
    "plt.plot(mu_sum['timestamp'], mu_sum['median'], color='blue')\n",
    "plt.fill_between(mu_sum['timestamp'], mu_sum['ci_2.5%'], mu_sum['ci_97.5%'], color='blue', alpha=0.2)\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"log(wind speed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnaEL43Jf2FR"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.plot(mu_sum['timestamp'], mu_sum['wind_speed'],color='black', alpha=0.5)\n",
    "plt.plot(mu_sum['timestamp'], mu_sum['median_original'], color='blue')\n",
    "plt.fill_between(mu_sum['timestamp'], mu_sum['ci_2.5%_original'], mu_sum2['ci_97.5%_original'], color='blue', alpha=0.2)\n",
    "#plt.set_title(\"Life Expectancy vs log(GDP per capita)\")\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"wind_speed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEFWcYwpFV6N"
   },
   "source": [
    "Let's now switch our focus on the predictive interval. \n",
    "Check the coverage of the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uhqy7wsRL-pG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predictival interval with the observations (both log scale and orginal scale) against timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNa0y-Pmi5HE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWFJS_iQGZpP"
   },
   "source": [
    "## 5. Investigating Our Research Questions  \n",
    "\n",
    "1. **Average Percentage Change Between Maximum and Minimum Wind Speed:**  \n",
    "   - Calculate the average percentage change between the maximum and minimum wind speed within a day and estimate its credible interval.  \n",
    "\n",
    "2. **Determining the Appropriate Polynomial Order:**  \n",
    "   - Evaluate the suitability of including an additional fourth-order polynomial term ($X_{i25}^4$) by comparing it with a model that includes only first to third-order polynomial terms.  \n",
    "   - Use model comparison metrics such as WAIC (Watanabe-Akaike Information Criterion) or LOOCV (Leave-One-Out Cross-Validation) to assess model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI3zYJe9dMqU"
   },
   "source": [
    "### 5.1 Average diurnal pattern\n",
    "To answer the first question, we can start by plotting the average diurnal pattern in log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9bqnLFTcHah"
   },
   "outputs": [],
   "source": [
    "# extract posterior draws of the relevant coefficient 'beta_hour'\n",
    "beta_sum = az.summary(model1_fit_az_idata, var_names=['beta_hour'], stat_funcs=custom_stats, extend=False)\n",
    "beta_sum['hour'] = range(0,24)\n",
    "beta_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99fnURrQ7kNT"
   },
   "outputs": [],
   "source": [
    "# plot the median and 95% credible bands against hour of the day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQFwP7updR5L"
   },
   "source": [
    "We calculate the average percentage change and compute its credible interval. Before proceeding, we double-check the lowest and highest wind speed values along with their corresponding times of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJqJwUwvrZ7a"
   },
   "outputs": [],
   "source": [
    "print(beta_sum.iloc[:24,0].max(),beta_sum.iloc[:24,0].idxmax())\n",
    "print(beta_sum.iloc[:24,0].min(), beta_sum.iloc[:24,0].idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NqgR0QoQSWU"
   },
   "source": [
    "We can obtain posterior draws using the `az.extract(idata, varname=[])` function. Since we now know that wind speed is lowest at 5 AM and peaks 12 hours later at 5 PM (17:00), we proceed by extracting the posterior draws of $\\beta_{hour5}$ and $\\beta_{hour17}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUuc8hxqqn3J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8HZK-CVZ6F5"
   },
   "source": [
    "Since wind speed is modeled on a log scale, we take the exponent of each posterior draw of\n",
    "$\\beta_{hour5}$ and $\\beta_{hour17}$\n",
    "to interpret the change in wind speed on the original scale. The percentage change from the lowerst to highest can be computed by\n",
    "$$\n",
    "(\\exp(\\beta_{hour17}-\\beta_{hour5})-1)\\times 100\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VybnW6_ptFd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV5sF1ePpsKk"
   },
   "source": [
    "### 5.2 Increasing the order of polynomial\n",
    "Implements Model 2, where a fourth-order polynomial term is added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zyfh1qdQpJhQ"
   },
   "outputs": [],
   "source": [
    "# Update data dictionary\n",
    "\n",
    "# Sample from the joint posterior using CmdStanPy\n",
    "\n",
    "# Save the output to a pickle file (optional)\n",
    "\n",
    "# quick check, but we should check the traceplots and pair plots as well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary stats including $\\hat{R}$ and ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_9xaW0XqLuf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3GHz0PEq37z"
   },
   "source": [
    "We will quickly check the fit by plotting the median of the posterior and get waic and loo-cv error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byq_gURzsmi4"
   },
   "outputs": [],
   "source": [
    "def get_mu_summary(mod):\n",
    "    idata = az.from_cmdstanpy(mod)\n",
    "    mu_sum = az.summary(idata, var_names=['mu'], stat_funcs=custom_stats, extend=False)\n",
    "    exp_mu_sum = az.summary(idata, var_names=['mu'], stat_funcs=custom_stats_original_sclae, extend=False)\n",
    "    mu_sum = pd.concat([mu_sum, exp_mu_sum], axis=1)\n",
    "    # Append the data to the summary\n",
    "    mu_sum['wind_speed'] = df_diurnal['wind_speed'].values\n",
    "    mu_sum['log_wind_speed'] = df_diurnal['log_wind_speed'].values\n",
    "    mu_sum['timestamp'] = df_diurnal['timestamp'].values\n",
    "    # Sort data\n",
    "    mu_sum = mu_sum.sort_values('timestamp')\n",
    "    return mu_sum\n",
    "\n",
    "def extract_elpd_loo(model):\n",
    "    idata = az.from_cmdstanpy(model)\n",
    "    loo = az.loo(idata, pointwise=True)\n",
    "    waic = az.waic(idata, pointwise=True)\n",
    "\n",
    "    df = pd.DataFrame({'elpd_loo': loo['elpd_loo'], 'elpd_waic': waic['elpd_waic']}, index=[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mor-XuU4qgEs"
   },
   "outputs": [],
   "source": [
    "mu_sum_model2 = get_mu_summary(model2_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDiOwkxxsN9O"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "ax.plot(mu_sum['timestamp'], mu_sum['wind_speed'], color='black', alpha=0.5,)\n",
    "ax.plot(mu_sum['timestamp'], mu_sum['median_original'], label='P = 3')\n",
    "ax.plot(mu_sum_model2['timestamp'], mu_sum_model2['median_original'], label='P = 4')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xBfp9avLUku"
   },
   "source": [
    "Is there a noticable  difference? Now we compare two models using LOOCV and WAIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcOePda9eGbt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model do you prefer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MfsPdaCywk4"
   },
   "source": [
    "### 6 Discussion\n",
    "\n",
    "Any limitaions you noticed and possible solutions to them."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
