{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLGlobalHealth/StatML4PopHealth/blob/main/lectures/day3/hsgp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDKRpPW8NO3y"
      },
      "source": [
        "# Scalable Gaussian Process Regression with Stan\n",
        "\n",
        "## Objectives\n",
        "In this practical, you will learn about how to implement an fast and scalable Gaussian Process (GP) approximation in Stan. More specificially, this practical will teach you how to implement the Hilbert space approximate Gaussian process (HSGP) proposed by Arno Solin and Simo Sarkkar in 2020 [1] and picked up by Riutort-Mayol et al. (2022) [2] for use in probabilistic programming frameworks such as Stan.\n",
        "\n",
        "1. Arno Solin and Simo Sarkka (2020). Hilber space methods for reduced-rank Gaussian process regression. *Statistics and Computing*.\n",
        "2. Gabriel Riutort-Mayol et al. (2022), Practical Hilbert space approximate Gaussian processes for probabilistic programming. *Statistics and Computing*.\n",
        "\n",
        "By the end of this practical,\n",
        "1. You will have a better understanding of implementing custom functions in Stan;\n",
        "2. You will improve your ability to translate mathematics into Stan code;\n",
        "3. You will have a better understanding of HSGP and its implementation in Stan.\n",
        "\n",
        "## Flow of the practical\n",
        "1. Review of Hilbert Space approximate Gaussian Processes\n",
        "2. How to implement HSGP in Stan\n",
        "3. An application of GPs for causal inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CmdStanPy for Google Colab\n",
        "!curl -O \"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/scripts/utilities.py\"\n",
        "from utilities import custom_install_cmdstan, test_cmdstan_installation\n",
        "custom_install_cmdstan()"
      ],
      "metadata": {
        "id": "EqaInM48Aopy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByOwbfbrASBw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from cmdstanpy import CmdStanModel\n",
        "import arviz as az\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualisation defaults\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rc('font', size=9)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=10)    # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=9)     # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=9)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=9)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=9)    # legend fontsize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFenMVwjJKnm"
      },
      "source": [
        "## The dataset\n",
        "For this tutorial, we will use the `nile` dataset available via the `statsmodels` library. The dataset consists of annual flow measurements of the Nile River at Aswan from 1871 to 1970."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "891zAZauASBx"
      },
      "outputs": [],
      "source": [
        "from statsmodels.datasets import nile\n",
        "nile = nile.load_pandas().data\n",
        "\n",
        "# Plot the data\n",
        "fig, ax = plt.subplots()\n",
        "nile.plot(x='year', y='volume', ax=ax)\n",
        "ax.set_title('Nile River Volume')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Volume (10^8 m^3)')\n",
        "ax.set_xlim(1871, 1970)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mST2_a7lJdwo"
      },
      "source": [
        "### Data preprocessing\n",
        "To make things easier for the model, we will normalise the year data to be between 0 and 1 and standardise the flow data to have a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIBtO01YJRMc"
      },
      "outputs": [],
      "source": [
        "volume = nile['volume'].values\n",
        "year = nile['year'].values\n",
        "\n",
        "# Standardise year\n",
        "year_mean = year.mean()\n",
        "year_std = year.std()\n",
        "x = (year - year_mean) / year_std\n",
        "\n",
        "# Standardise volume\n",
        "volume_mean = volume.mean()\n",
        "volume_std = volume.std()\n",
        "y = (volume - volume_mean) / volume_std\n",
        "\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNfu509CASBy"
      },
      "source": [
        "## Review of Hilbert Space approximate Gaussian Processes\n",
        "\n",
        "Solin and Sarkka (2020) proposed to approximate stationary kernels as a truncated sum of the spectral density of the kernel evaluated at the square root of specific eigenvalues multiplied by the corresponding eigenvectors:\n",
        "$$\n",
        "k(x,x') \\approx \\sum_{m=1}^M S_\\theta(\\sqrt{\\lambda_m}) \\phi_m(x) \\phi_m(x'),\n",
        "$$\n",
        "where $S_\\theta$ is the spectral density of the kernel, $\\lambda_m$ are the eigenvalues, and $\\phi_m$ are the eigenvectors. The number of terms $M$ is a hyperparameter that controls the approximation accuracy.\n",
        "\n",
        "Here, the eigenvalues and eigenvectors are given as,\n",
        "$$\n",
        "\\lambda_m = \\left( \\frac{n\\pi}{2L} \\right)^2, \\quad \\phi_m(x) = \\sqrt{\\frac1L} \\sin\\left(\\sqrt{\\lambda_m}(x + L)\\right).\n",
        "$$\n",
        "Here $L$ is a boundary condition which determines the size of the domain in which the GP is approximated.\n",
        "\n",
        "The expression for the spectral density depends on the kernel. Here are 3 of the most commonly used stationary kernels and their spectral densities:\n",
        "\n",
        "1. Squared exponential kernel:\n",
        "$$\n",
        "S_{\\sigma,\\ell}(\\omega) = \\sigma^2 \\sqrt{2\\pi} \\ell \\exp\\left(-\\frac{\\ell^2\\omega^2}{2}\\right).\n",
        "$$\n",
        "2. Matern 3/2 kernel:\n",
        "$$\n",
        "S_{\\sigma,\\ell}(\\omega) = \\sigma^2 \\frac{2\\sqrt{3}}{\\ell} \\left(\\frac{3}{\\ell^2} + \\omega^2 \\right)^{-2}.\n",
        "$$\n",
        "3. Matern 5/2 kernel:\n",
        "$$\n",
        "S_{\\sigma,\\ell}(\\omega) = \\sigma^2 \\frac{8\\sqrt{5}}{3\\ell} \\left(\\frac{5}{\\ell^2} + \\omega^2 \\right)^{-5/2}.\n",
        "$$\n",
        "\n",
        "We can rewrite the truncated sum in matrix notation as\n",
        "$$\n",
        "k(x,x') \\approx \\mathbf{\\phi}(x)^\\top \\mathbf{\\Delta} \\mathbf{\\phi}(x'),\n",
        "$$\n",
        "where $\\mathbf{\\phi}(x) = \\{ \\phi_m(x) \\}_{m=1}^M \\in \\mathbb{R}^M$ are the eigenvectors evaluated at the input point, and $\\mathbf{\\Delta} = \\text{diag}(\\{ S_\\theta(\\sqrt{\\lambda_m}) \\}_{m=1}^M) \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix of the spectral density evaluated at the square root of the eigenvalues.\n",
        "\n",
        "Then, the covariance matrix $\\mathbf{K}$ can be approximated as\n",
        "$$\n",
        "\\mathbf{K} \\approx \\tilde{\\mathbf{K}} = \\mathbf{\\Phi} \\mathbf{\\Delta} \\mathbf{\\Phi}^\\top,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\mathbf{\\Phi} = \\begin{pmatrix}\n",
        "\\phi_1(x_1) & \\cdots & \\phi_M(x_1) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_1(x_n) & \\cdots & \\phi_M(x_n)\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "is a matrix of the eigenvectors. From this we have that the GP sample can be approximated as\n",
        "$$\n",
        "\\mathbf{f} \\approx \\tilde{\\mathbf{f}} \\sim N(\\mathbf{0}, \\tilde{\\mathbf{K}}).\n",
        "$$\n",
        "We can sample from the approximate GP by first sampling an auxiliary vector $\\mathbf{z} \\sim N(\\mathbf{0}, \\mathbf{I}_M)$ and then computing the approximate GP sample as\n",
        "$$\n",
        "\\tilde{\\mathbf{f}} = \\mathbf{\\Phi} \\mathbf{\\Delta}^{1/2} \\mathbf{z}\n",
        "$$\n",
        "where $\\mathbf{\\Delta}^{1/2}$ is the square root of the diagonal matrix $\\mathbf{\\Delta}$:\n",
        "$$\n",
        "\\mathbf{\\Delta}^{1/2} = \\begin{pmatrix}\n",
        "S_\\theta(\\sqrt{\\lambda_1})^{1/2} & & \\\\\n",
        "& \\ddots & \\\\\n",
        "& & S_\\theta(\\sqrt{\\lambda_M})^{1/2}\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "<details>\n",
        "<summary>Click to see the proof</summary>\n",
        "\n",
        "Let $\\tilde{\\mathbf{L}} = \\mathbf{\\Phi} \\mathbf{\\Delta}^{1/2}$. Then, we have that\n",
        "$$\n",
        "\\tilde{\\mathbf{K}} = \\mathbf{\\Phi}^\\top \\mathbf{\\Delta} \\mathbf{\\Phi} = \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^\\top.\n",
        "$$\n",
        "Let $\\mathbf{z} \\sim N(\\mathbf{0}, \\mathbf{I}_M)$. Then, we have that\n",
        "$$\n",
        "\\mathbb{E}[\\mathbf{z}] = \\mathbf{0}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\mathbb{E}[\\tilde{\\mathbf{L}} \\mathbf{z}] = \\mathbf{\\Phi} \\mathbf{\\Delta}^{1/2} \\mathbb{E}[\\mathbf{z}] = \\mathbf{0}.\n",
        "$$\n",
        "Thus, $\\tilde{\\mathbf{L}} \\mathbf{z}$ has zero mean. The covariance matrix of $\\tilde{\\mathbf{L}} \\mathbf{z}$ is\n",
        "$$\n",
        "\\text{Cov}(\\tilde{\\mathbf{L}} \\mathbf{z})\n",
        "= \\mathbb{E}[(\\tilde{\\mathbf{L}} \\mathbf{z}) (\\tilde{\\mathbf{L}}\\mathbf{z})^\\top] = \\tilde{\\mathbf{L}}\\mathbb{E}[ \\mathbf{z} \\mathbf{z}^\\top]\\tilde{\\mathbf{L}}^\\top.\n",
        "$$\n",
        "Since $\\mathbf{z} \\sim N(\\mathbf{0}, \\mathbf{I}_M)$, we have that\n",
        "$$\n",
        "\\mathbb{E}[\\mathbf{z} \\mathbf{z}^\\top] = \\mathbf{I}_M.\n",
        "$$\n",
        "Thus,\n",
        "$$\n",
        "\\text{Cov}(\\tilde{\\mathbf{L}} \\mathbf{z}) = \\tilde{\\mathbf{L}}\\tilde{\\mathbf{L}}^\\top = \\tilde{\\mathbf{K}}.\n",
        "$$\n",
        "This confirms that $\\tilde{\\mathbf{f}} \\sim N(\\mathbf{0}, \\tilde{\\mathbf{K}})$ and concludes the proof.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The boundary condition\n",
        "HSGP is an approximation of a Gaussian process on a compact subspace of the real line $S \\subset \\mathbb{R}$. We are going to assume that our input points are centered around 0 and we are going to define our subspace as $[-L, L]$ where\n",
        "$$\n",
        "L = C \\times \\max(\\mathbf{x}).\n",
        "$$\n",
        "We refer to $C$ as the boundary inflation factor and for this tutorial set $C = 1.5$."
      ],
      "metadata": {
        "id": "oaCo74X0CA4R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fsZ3XTNASBy"
      },
      "source": [
        "## The model\n",
        "\n",
        "Let $\\mathbf{y} = (y_1,\\ldots,y_n)^\\top$ be a vector of outcomes, in this case flow volume. Let $\\mathbf{x} = (x_1,\\ldots,x_n)^\\top$ be a vector of inputs, in this case year. We will model the data as\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{y} &= \\alpha + \\tilde{f}(\\mathbf{x}) + \\boldsymbol{\\varepsilon}, \\\\\n",
        "\\boldsymbol{\\varepsilon} &\\sim N(0, \\sigma_{\\varepsilon}^2\\mathbf{I}_n), \\\\\n",
        "\\tilde{f}(\\mathbf{x}) &\\sim \\text{HSGP}(\\mathbf{x}; \\mathbf{z}, \\sigma, \\ell), \\\\\n",
        "\\alpha &\\sim N(0, 10), \\\\\n",
        "\\boldsymbol{z} &\\sim N(0, \\mathbf{I}_M), \\\\\n",
        "\\sigma &\\sim \\text{inv-Gamma}(5, 5) \\\\\n",
        "\\ell &\\sim \\text{inv-Gamma}(5, 5) \\\\\n",
        "\\sigma_{\\varepsilon} &\\sim \\text{inv-Gamma}(5,5)\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nSdvbKdASBy"
      },
      "source": [
        "## Implementing HSGP in Stan\n",
        "\n",
        "We will work in a new Stan file: call it `hsgp_regression.stan`. The Stan program will be similar in structure to the GP program, but with a few modifications. We will begin by implementing functions to construct the spectral densities, eigenvalues, and eigenvectors. We will put them together to define a functions named `hsgp_se`, `hsgp_matern32`, and `hsgp_matern52`, that take in the input points `x`, the hyperparameters `sigma` and `ell`, a matrix of eigenvectors `PHI`, and a vector of auxiliary variables `z`. The function will return the HSGP sample `f`.\n",
        "\n",
        "### Spectral density functions\n",
        "\n",
        "Let us start by defining the spectral densities. We will define three functions: `spd_se`, `spd_matern32`, and `spd_matern52`. These functions will take as input a vector of frequencies `omega` and the hyperparameters `sigma` and `ell`. The functions will return the spectral density evaluated at the frequencies.\n",
        "\n",
        "```stan\n",
        "functions {\n",
        "\tvector spd_se(vector omega, real sigma, real ell) {\n",
        "\t\t// Implement the spectral density for the squared exponential kernel\n",
        "\t}\n",
        "\n",
        "\tvector spd_matern32(vector omega, real sigma, real ell) {\n",
        "\t\t// Implement the spectral density for the Matern 3/2 kernel\n",
        "\t}\n",
        "\n",
        "\tvector spd_matern52(vector omega, real sigma, real ell) {\n",
        "\t\t// Implement the spectral density for the Matern 5/2 kernel\n",
        "\t}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfGvxL8_ASBy"
      },
      "source": [
        "### Eigenvalues and eigenvector functions\n",
        "Next, we will implement functions to compute the eigenvalues and eigenvectors. We will define two functions: `eigenvalues` and `eigenvectors`. The `eigenvalues` function will take in the number basis functions `M` and the boundary condition `L` and return a vector of eigenvalues. The `eigenvectors` function will take in the input points `x`, the number of basis functions `M`, the boundary condition `L`, and the eigenvalues `lambda` and return a matrix of eigenvectors.\n",
        "\n",
        "```stan\n",
        "functions {\n",
        "\t// Other functions...\n",
        "\n",
        "\tvector eigenvalues(int M, real L) {\n",
        "\t\t// Implement the eigenvalues function\n",
        "\t}\n",
        "\n",
        "\tmatrix eigenvectors(vector x, int M, real L, vector lambda) {\n",
        "\t\t// Implement the eigenvectors function\n",
        "\t}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT8ROc5aASBz"
      },
      "source": [
        "### HSGP function\n",
        "Finally, we put these components together to define the `hsgp` function. The function will take in the input points `x`, the hyperparameters `sigma` and `ell`, the eigenvector matrix `PHI`, and a vector of auxiliary variables `z`. The function will return a vector of the HSGP sample `f`.\n",
        "\n",
        "```stan\n",
        "functions {\n",
        "\t// Other functions...\n",
        "\n",
        "\tvector hsgp_se(vector x, real sigma, real ell, vector lambdas, matrix PHI, vector z) {\n",
        "\t\tint n = rows(x);\n",
        "\t\tint M = cols(PHI);\n",
        "\t\tvector[n] f;\n",
        "\t\tmatrix[M, M] Delta;\n",
        "\n",
        "\t\t// Implement the HSGP function\n",
        "\t\t// 1. Compute the spectral densities\n",
        "\n",
        "\t\t// 2. Construct the diagonal matrix Delta\n",
        "\n",
        "\t\t// 3. Compute the HSGP sample\n",
        "\n",
        "\t\treturn f;\n",
        "\t}\n",
        "\n",
        "\tvector hsgp_matern32(vector x, real sigma, real ell, matrix PHI, vector z) {\n",
        "\t\t// Implement the HSGP function\n",
        "\t}\n",
        "\n",
        "\tvector hsgp_matern52(vector x, real sigma, real ell, matrix PHI, vector z) {\n",
        "\t\t// Implement the HSGP function\n",
        "\t}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tO3n0HJASBz"
      },
      "source": [
        "### Data block\n",
        "We will need to define the data block to include the number of data points `N` (positive integer), the input points `x` (vector of length `N`), the outcome `y` (real valued array of length `N`), the boundary inflation constant `C` (positive real value), and the number of eigenfunctions `M` (positive integer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XALEc0Z_ASBz"
      },
      "source": [
        "### Transformed data block\n",
        "In the transformed data block:\n",
        "1. Define the boundary condition `L` as `C * max(x)`\n",
        "2. Precompute the eigenvalues `lambda` and the eigenvectors `PHI` using the `eigenvalues` and `eigenvectors` functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honUWuqjASBz"
      },
      "source": [
        "### Parameters block\n",
        "In the parameters block define:\n",
        "1. The interecpt `alpha` (real value)\n",
        "2. The noise standard deviation `sigma_eps` (positive real value)\n",
        "3. vector of standard normal random variables `z`\n",
        "4. The marginal GP variance `sigma` (positive real)\n",
        "5. The GP lengthscale `ell` (positive real)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformed parameters block\n",
        "In the transformed parameters block, implement\n",
        "1. `f`: a vector of size `N` which is an approximate sample of a GP with the squared exponential kernel.\n",
        "2. `mu`: a vector of size `N` that contains the expected value of `y` at each input point."
      ],
      "metadata": {
        "id": "ip2Bio3yGKJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model block\n",
        "In the model block we assign priors to all the parameters defined in the parameters block and we define the likelihood."
      ],
      "metadata": {
        "id": "I3IM0S-GGEtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated quantities block\n",
        "Finally, in the generated quatities block you compute the log likelihood `log_lik` for each data point as well as generate random samples `y_rep` based on the inferred parameters of the model."
      ],
      "metadata": {
        "id": "9J8c8ff0H3Cd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJENTW4KASBz"
      },
      "source": [
        "Compile the Stan code using the `CmdStanModel` class from the `cmdstanpy` library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsgp_se_model = CmdStanModel(stan_file='hsgp_regression.stan')"
      ],
      "metadata": {
        "id": "yILkgeSaJJ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD7yTHrEASBz"
      },
      "source": [
        "Create the stan data dictionary with the number of data points `N`, the input points `x`, the output points `y`, the number of basis functions `M`, the boundary inflation factor `C`, and the number of eigenvectors `M`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob195Q9OASBz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MCMC algorithm with 4 chains, 500 warmup iterations, and 1000 iterations. Set the `adapt_delta` argument to 0.95 and set the random seed."
      ],
      "metadata": {
        "id": "kMTnvO-wLy7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvImno5WASB0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# ====================\n",
        "# Your code here\n",
        "# ====================\n",
        "\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(f\"Runtime of the Stan model: {runtime} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `diagnose` method to perform a quick diagnosis of the model."
      ],
      "metadata": {
        "id": "ElGx_GTROsnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWsytsyPASB0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a dictionary `custom_summary_fns` of lambda functions that calculates the median, the 2.5% quantile and the 97.5% quantile."
      ],
      "metadata": {
        "id": "xuBy9XdvO8mn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCah2iXCJr2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the fit object to an ArviZ inference data object using\n",
        "the `from_cmdstanpy` function. Use the `summary` method and `custom_summary_fns`, summarise the posterior distribution of `mu` and `y_rep`"
      ],
      "metadata": {
        "id": "lAVSnlPrPFQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRqCohfKASB0"
      },
      "outputs": [],
      "source": [
        "idata_hsgp_se = az.from_cmdstanpy(hsgp_se_fit)\n",
        "mu_hsgp_se_sum = az.summary(idata_hsgp_se,\n",
        "                            var_names=['mu'],\n",
        "                            stat_funcs=custom_summary_fns,\n",
        "                            extend=False)\n",
        "y_rep_hsgp_se_sum = az.summary(idata_hsgp_se,\n",
        "                               var_names=['y_rep'],\n",
        "                               stat_funcs=custom_summary_fns,\n",
        "                               extend=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that plots the posterior summary against the observed data\n",
        "def plot_posterior_summary(year, y, posterior_summary):\n",
        "\tfig, ax = plt.subplots()\n",
        "\tax.scatter(year, y, label='Data', s=10)\n",
        "\tax.plot(year, posterior_summary['median'], label='Posterior Median of f', color='red')\n",
        "\tax.fill_between(year, posterior_summary['q2.5'], posterior_summary['q97.5'], color='gray', alpha=0.2, label='95% CI')\n",
        "\tax.set_title('Posterior Median of f vs Data')\n",
        "\tax.set_xlabel('Year')\n",
        "\tax.set_ylabel('Volume (10^8 m^3)')\n",
        "\tax.set_xlim(1871, 1970)\n",
        "\tax.legend()\n",
        "\tplt.show()"
      ],
      "metadata": {
        "id": "ZYYnwX6iKCzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y178O9xJASB0"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, mu_hsgp_se_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOuqLrWKASB0"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, y_rep_hsgp_se_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Stan file titled `hsgp_matern32_regression.stan` where the squared exponential covariance kernel is replaced by the Matern 3/2 kernel."
      ],
      "metadata": {
        "id": "CBtGGho6Llca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--D8Cp3ASB1"
      },
      "outputs": [],
      "source": [
        "hsgp_matern32_model = CmdStanModel(stan_file='hsgp_matern32_regression.stan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8_rp9D5ASB1"
      },
      "outputs": [],
      "source": [
        "hsgp_matern32_fit = hsgp_matern32_model.sample(\n",
        "\tdata=stan_data,\n",
        "\tchains=4,\n",
        "\titer_warmup=500,\n",
        "\titer_sampling=1000,\n",
        "\tadapt_delta=0.95,\n",
        "\tseed=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ5QNRmRASB1"
      },
      "outputs": [],
      "source": [
        "idata_hsgp_matern32 = az.from_cmdstanpy(hsgp_matern32_fit)\n",
        "mu_hsgp_matern32_sum = az.summary(idata_hsgp_matern32, var_names=['mu'], stat_funcs=custom_summary_fns, extend=False)\n",
        "y_rep_hsgp_matern32_sum = az.summary(idata_hsgp_matern32, var_names=['y_rep'], stat_funcs=custom_summary_fns, extend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEiFwEPkASB1"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, mu_hsgp_matern32_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dasvGkNwASB1"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, y_rep_hsgp_matern32_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Stan file titled `hsgp_matern52_regression.stan` where the squared exponential covariance kernel is replaced by the Matern 5/2 kernel."
      ],
      "metadata": {
        "id": "uBYgMMsNMr1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRzdD9-WASB2"
      },
      "outputs": [],
      "source": [
        "hsgp_matern52_model = CmdStanModel(stan_file='hsgp_matern52_regression.stan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFUmwXEeASB2"
      },
      "outputs": [],
      "source": [
        "hsgp_matern52_fit = hsgp_matern52_model.sample(\n",
        "\tdata=stan_data,\n",
        "\tchains=4,\n",
        "\titer_warmup=500,\n",
        "\titer_sampling=1000,\n",
        "\tadapt_delta=0.95,\n",
        "\tseed=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDLLrShKASB2"
      },
      "outputs": [],
      "source": [
        "idata_hsgp_matern52 = az.from_cmdstanpy(hsgp_matern52_fit)\n",
        "mu_hsgp_matern52_sum = az.summary(idata_hsgp_matern52, var_names=['mu'], stat_funcs=custom_summary_fns, extend=False)\n",
        "y_rep_hsgp_matern52_sum = az.summary(idata_hsgp_matern52, var_names=['y_rep'], stat_funcs=custom_summary_fns, extend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhkIsOqkASB2"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, mu_hsgp_matern52_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa4WIqlpASB2"
      },
      "outputs": [],
      "source": [
        "plot_posterior_summary(year, y, y_rep_hsgp_matern52_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-emqHb5ASB2"
      },
      "source": [
        "## Application of GPs\n",
        "\n",
        "Did you notice that there seems to a a significant and permanent drop in the flow volume of the Nile river around 1899? This is attributed to several factors low rain fall in the Ethiopian highlands, and the construction of the Aswan Dam in 1902. Modeling the time series of flow volume using a stationary GP is clearly not appropriate in this case. Upon visual examination, it seems that if we split the data around 1899, and fit two separate GPs to the data, we might be able to model the data more appropriately. Let us alter our model to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvs2zc8GASB2"
      },
      "source": [
        "Let $t = 1, \\ldots, T$ be the time index. We will split the data into two parts: $t=1,\\ldots,c-1$ and $t=c,\\ldots,T$, where $c = 1899$. We will fit a GP to each part of the data. The likelihood for the data is then\n",
        "$$\n",
        "\\begin{align*}\n",
        "Y_t \\sim \\begin{cases}\n",
        "N(\\mu_0(t), \\sigma_{\\varepsilon}^2) & \\text{if } t < c, \\\\\n",
        "N(\\mu_1(t), \\sigma_{\\varepsilon}^2) & \\text{if } t \\geq c.\n",
        "\\end{cases}\n",
        "\\end{align*}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu_0(t) &= \\alpha_0 + f_0(t) \\\\\n",
        "\\mu_1(t) &= \\alpha_1 + f_1(t).\n",
        "\\end{align*}\n",
        "$$\n",
        "We will assign the following priors\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\alpha_0, \\alpha_1 &\\sim N(0, 1), \\\\\n",
        "\\sigma_{\\varepsilon} &\\sim \\text{Inv-Gamma}(5, 5) \\\\\n",
        "f_0 &\\sim \\text{HSGP}(0, k_0(t,t';\\sigma_0,\\ell_0)), \\\\\n",
        "f_1 &\\sim \\text{HSGP}(0, k_1(t,t';\\sigma_1,\\ell_1)), \\\\\n",
        "\\sigma_0, \\sigma_1 &\\sim \\text{Inv-Gamma}(5, 5), \\\\\n",
        "\\ell_0, \\ell_1 &\\sim \\text{Inv-Gamma}(5, 5)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6km73VGLASB3"
      },
      "outputs": [],
      "source": [
        "hsgp_rdd = CmdStanModel(stan_file='hsgp_rdd.stan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U-kwzB9ASB3"
      },
      "outputs": [],
      "source": [
        "tid0 = np.where(year < 1899)[0] + 1 # Python is 0-indexed, Stan is 1-indexed\n",
        "tid1 = np.where(year >= 1899)[0] + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGX2LVMWASB3"
      },
      "outputs": [],
      "source": [
        "stan_data = {\n",
        "\t'N': len(x),\n",
        "  'N0': len(tid0),\n",
        "  'N1': len(tid1),\n",
        "\t't': x,\n",
        "  'tid0': tid0,\n",
        "  'tid1': tid1,\n",
        "\t'y': y,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2cQCKkVASB3"
      },
      "outputs": [],
      "source": [
        "hsgp_rdd_fit = hsgp_rdd.sample(\n",
        "    data=stan_data,\n",
        "    chains=4,\n",
        "    iter_warmup=500,\n",
        "    iter_sampling=1000,\n",
        "    adapt_delta=0.95,\n",
        "    seed=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKVo2WH0ASCA"
      },
      "outputs": [],
      "source": [
        "print(hsgp_rdd_fit.diagnose())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPHOXcYBASCA"
      },
      "outputs": [],
      "source": [
        "idata_rdd = az.from_cmdstanpy(hsgp_rdd_fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhYu9Aa5ASCA"
      },
      "outputs": [],
      "source": [
        "mu0_sum = az.summary(idata_rdd, var_names=['mu0'], stat_funcs=custom_summary_fns, extend=False)\n",
        "mu1_sum = az.summary(idata_rdd, var_names=['mu1'], stat_funcs=custom_summary_fns, extend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbILFMHIASCB"
      },
      "outputs": [],
      "source": [
        "mu_sum = pd.concat((\n",
        "    mu0_sum.iloc[tid0 - 1,],\n",
        "    mu1_sum.iloc[tid1 - 1,]\n",
        "))\n",
        "\n",
        "plot_posterior_summary(year, y, mu_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuOLW-BEASCB"
      },
      "source": [
        "Suppose we wanted to quantify the change in the flow volume of the Nile river around 1899. There is a famous framework in the realm of causal inference called the regression discontinuity design (RDD) that can be used to estimate the causal effect of an intervention. The RDD framework is based on the idea that the treatment effect is discontinuous at the threshold. In our case, the threshold is the year 1899. The goal is to estimate the following quantity\n",
        "$$\n",
        "\\begin{align}\n",
        "\\tau_{\\text{sharp}} &= \\lim_{\\epsilon \\to 0} \\left( \\mathbb{E}[Y_{c + \\epsilon} | t = c + \\epsilon] - \\mathbb{E}[Y_{c - \\epsilon} | t = c - \\epsilon] \\right) \\\\\n",
        "&= \\mu_1(c) - \\mu_0(c).\n",
        "\\end{align}\n",
        "$$\n",
        "Recall that we do not have data for the case where the intervention did not occur at time $t=c$. However, we can use the GP up to timepoint $c-1$ to make predictions for the flow volume at time point $c$ and use that to quantify the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsltxadoASCB"
      },
      "outputs": [],
      "source": [
        "mu0_flat = idata_rdd.posterior.mu0.stack(sample=('chain', 'draw'))\n",
        "mu1_flat = idata_rdd.posterior.mu1.stack(sample=('chain', 'draw'))\n",
        "\n",
        "diff = (mu1_flat[0,:] - mu0_flat[tid1[0],:])\n",
        "diff = diff * volume_std # Reverse the standardisation to get the difference in volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llT9Tj6uASCB"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(diff, fill=True)\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.axvline(diff.mean(), color='red', linestyle='--')\n",
        "plt.title('KDE of Difference in Flow Volume')\n",
        "plt.xlabel('Difference')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BjHBvJ8ASCB"
      },
      "source": [
        "From the plot above, we can see that the average flow volume decreased by approximately 300 units around 1899. We can also plot the flow volume predictions until 1970 if the intervention did not occur (counter factuals)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-zshX3SASCB"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "ax.scatter(year, y, color='black', label='Data', s=10)\n",
        "ax.plot(year, mu0_sum['median'], color='blue')\n",
        "ax.fill_between(year, mu0_sum['q2.5'], mu0_sum['q97.5'], color='blue', alpha=0.2)\n",
        "ax.plot(year[tid1-1], mu1_sum['median'][tid1-1], color='red')\n",
        "ax.fill_between(year[tid1-1], mu1_sum['q2.5'][tid1-1], mu1_sum['q97.5'][tid1-1], color='red', alpha=0.2)\n",
        "\n",
        "ax.axvline(1899, color='black', linestyle='--')\n",
        "ax.set_xlim(1871, 1970)\n",
        "\n",
        "ax.set_ylabel('Volume (standardized)')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_title('Posterior mean of flow volume with counterfactuals')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqfSeQaBASCC"
      },
      "source": [
        "We observe that as we predict more into the future, the uncertainty increases, which is what we would expect."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}